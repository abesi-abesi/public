{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"情報メディア応用ゼミ_GoogleColaboratory深層学習編.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7dYXmHe0W2a1","colab_type":"text"},"source":["# 応用ゼミ 深層学習編\n","\n","情報メディア学科\n","\n","中島 克人（Katsuto Nakajima）\n","\n","nakajima@mail.ms.dendai.ac.jp\n","\n","Code Maintained by Akira Sekizawa (18fmi17@ms.dendai.ac.jp)\n","\n","# 教材の内容\n","\n","TensorFlow 2.0を用いて画像識別器の学習を行う．\n","\n","識別器には**DenseNet121**，**DenseNet169**，**DenseNet201**，**InceptionResNetV2**，**InceptionV3**，**MobileNet**，**MobileNetV2**，**NASNetLarge**，**NASNetMobile**，**ResNet50**，**VGG16**，**VGG19**，**Xception**が使用できる．\n","\n","学習アルゴリズムには**Adadelta**，**Adagrad**，**Adam**，**Adamax**，**Ftrl**，**Nadam**，**RMSprop**，**SGD**が使用できる．\n","\n","モデルや学習アルゴリズム，入力解像度や学習エポック数などのハイパーパラメータの調整を各自で試行錯誤することで，深層学習の理解を深めることを目的とする．\n","\n","また一般的なサンプルコードではカバーされていない，グレースケール画像を用いたモデルの学習や，ファインチューニングとフルチューニングの切り替え，転移学習とフルスクラッチ学習の切り替えに対応しており，それらについても理解を深めることができる．\n","\n","# 更新履歴\n","[9/6/19]\n","*   TensorFlow 2.0 Beta版からRelease Candidate版へと更新した\n","*   識別器にResNet 101, ResNet 151, ResNetV2 50, ResNetV2 101, ResNetV2 152)を追加した\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VnSeyF_ymTO2","colab_type":"text"},"source":["# 1. Google Driveのマウント\n","\n","Google Driveにあらかじめ学習対象の画像データをアップロードしておく．\n","\n","ディレクトリ構成を以下に示す．クラス名や画像のファイル名には任意の文字列を指定できる．画像形式はJPEGまたはPNGに対応する．\n","\n","**画像は学習時にネットワークの入力解像度に合わせてアスペクト比を維持して自動でリサイズされ，RGBとグレースケールもネットワークの入力タイプに合わせて自動変換される．**何も事前処理せずに画像をアップロードするだけでよい．\n","\n","\\<Rootディレクトリ\\>\n","  \n","　　\\<クラス1\\>\n","  \n","　　　　\\<画像1.jpg\\>\n","  \n","　　　　\\<画像2.jpg\\>\n","  \n","　　　　\\<画像3.png\\>\n","  \n","　　\\<クラス2\\>\n","  \n","　　　　\\<画像1.jpg\\>\n","  \n","　　　　\\<画像2.jpg\\>\n","  \n","　　　　\\<画像3.png\\>"]},{"cell_type":"code","metadata":{"id":"HL6w0m7t64ec","colab_type":"code","colab":{}},"source":["import google.colab.drive\n","google.colab.drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jxmqlMMm9iJ","colab_type":"text"},"source":["# 2. ライブラリのインストール"]},{"cell_type":"code","metadata":{"id":"8XXDzEEI173u","colab_type":"code","colab":{}},"source":["!pip install -q -U opencv-python\n","!pip install -q -U opencv-contrib-python\n","!pip install -q -U tensorflow-gpu==2.0.0\n","!pip install -q pathlib2\n","\n","# Google Colaboratoryでcv2.imshowを使うための特別な処理\n","import cv2\n","import google.colab.patches\n","cv2.imshow = lambda winname, mat: google.colab.patches.cv2_imshow(mat)\n","\n","# Kerasのpreprocess_input関数を使うための特別な処理\n","import tensorflow as tf\n","_ = tf.keras.applications.vgg16.preprocess_input(tf.zeros([1, 1, 1, 3]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nvJ99pofyOb","colab_type":"text"},"source":["# 3. ライブラリのインポート"]},{"cell_type":"code","metadata":{"id":"Y3l2gqfx5bYS","colab_type":"code","colab":{}},"source":["import random\n","import sys\n","import os\n","import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import pathlib2\n","from PIL import Image\n","import shutil\n","from tensorflow.keras.utils import plot_model\n","from keras.models import load_model\n","import keras.preprocessing\n","import pickle\n","import keras.preprocessing.image\n","import datetime\n","\n","tfds.disable_progress_bar()\n","MODELS = {\n","  'DenseNet121': (\n","    tf.keras.applications.DenseNet121,\n","    tf.keras.applications.densenet.preprocess_input,\n","    tf.keras.applications.densenet.decode_predictions),\n","  'DenseNet169': (\n","    tf.keras.applications.DenseNet169,\n","    tf.keras.applications.densenet.preprocess_input,\n","    tf.keras.applications.densenet.decode_predictions),\n","  'DenseNet201': (\n","    tf.keras.applications.DenseNet201,\n","    tf.keras.applications.densenet.preprocess_input,\n","    tf.keras.applications.densenet.decode_predictions),\n","  'InceptionResNetV2': (\n","    tf.keras.applications.InceptionResNetV2,\n","    tf.keras.applications.inception_resnet_v2.preprocess_input,\n","    tf.keras.applications.inception_resnet_v2.decode_predictions),\n","  'InceptionV3': (\n","    tf.keras.applications.InceptionV3,\n","    tf.keras.applications.inception_v3.preprocess_input,\n","    tf.keras.applications.inception_v3.decode_predictions),\n","  'MobileNet': (\n","    tf.keras.applications.MobileNet,\n","    tf.keras.applications.mobilenet.preprocess_input,\n","    tf.keras.applications.mobilenet.decode_predictions),\n","  'MobileNetV2': (\n","    tf.keras.applications.MobileNetV2,\n","    tf.keras.applications.mobilenet_v2.preprocess_input,\n","    tf.keras.applications.mobilenet_v2.decode_predictions),\n","  'NASNetLarge': (\n","    tf.keras.applications.NASNetLarge,\n","    tf.keras.applications.nasnet.preprocess_input,\n","    tf.keras.applications.densenet.decode_predictions),\n","  'NASNetMobile': (\n","    tf.keras.applications.NASNetMobile,\n","    tf.keras.applications.nasnet.preprocess_input,\n","    tf.keras.applications.nasnet.decode_predictions),\n","  'ResNet50': (\n","    tf.keras.applications.ResNet50,\n","    tf.keras.applications.resnet.preprocess_input,\n","    tf.keras.applications.resnet.decode_predictions),\n","  'ResNet101': (\n","    tf.keras.applications.ResNet101,\n","    tf.keras.applications.resnet.preprocess_input,\n","    tf.keras.applications.resnet.decode_predictions),\n","  'ResNet152': (\n","    tf.keras.applications.ResNet152,\n","    tf.keras.applications.resnet.preprocess_input,\n","    tf.keras.applications.resnet.decode_predictions),\n","  'ResNet50V2': (\n","    tf.keras.applications.ResNet50V2,\n","    tf.keras.applications.resnet_v2.preprocess_input,\n","    tf.keras.applications.resnet_v2.decode_predictions),\n","  'ResNet101V2': (\n","    tf.keras.applications.ResNet101V2,\n","    tf.keras.applications.resnet_v2.preprocess_input,\n","    tf.keras.applications.resnet_v2.decode_predictions),\n","  'ResNet152V2': (\n","    tf.keras.applications.ResNet152V2,\n","    tf.keras.applications.resnet_v2.preprocess_input,\n","    tf.keras.applications.resnet_v2.decode_predictions),\n","  'VGG16': (\n","    tf.keras.applications.VGG16,\n","    tf.keras.applications.vgg16.preprocess_input,\n","    tf.keras.applications.vgg16.decode_predictions),\n","  'VGG19': (\n","    tf.keras.applications.VGG19,\n","    tf.keras.applications.vgg19.preprocess_input,\n","    tf.keras.applications.vgg19.decode_predictions),\n","  'Xception': (\n","    tf.keras.applications.Xception,\n","    tf.keras.applications.xception.preprocess_input,\n","    tf.keras.applications.xception.decode_predictions),\n","}\n","OPTIMIZERS = {\n","  'Adadelta': tf.keras.optimizers.Adadelta,\n","  'Adagrad': tf.keras.optimizers.Adagrad,\n","  'Adam': tf.keras.optimizers.Adam,\n","  'Adamax': tf.keras.optimizers.Adamax,\n","  'Ftrl': tf.keras.optimizers.Ftrl,\n","  'Nadam': tf.keras.optimizers.Nadam,\n","  'RMSprop': tf.keras.optimizers.RMSprop,\n","  'SGD': tf.keras.optimizers.SGD,\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFdzkjPXT4tn","colab_type":"text"},"source":["# 4. サンプルデータのダウンロード"]},{"cell_type":"code","metadata":{"id":"PwyoBv7xUTjz","colab_type":"code","colab":{}},"source":["if not pathlib2.Path('/content/drive/My Drive/AppliedSeminar2019/SampleData').exists():\n","  !wget http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n","  !tar -xf 101_ObjectCategories.tar.gz\n","  !mkdir -p '/content/drive/My Drive/AppliedSeminar2019/SampleData'\n","  !mv 101_ObjectCategories '/content/drive/My Drive/AppliedSeminar2019/SampleData/Caltech101'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JO3yUj_mf1ve","colab_type":"text"},"source":["# 5. ハイパーパラメータの設定\n","\n","・**学習用画像のディレクトリ** (INPUT_DIR)：画像をクラス別のディレクトリに保存したディレクトリのパスを指定する\n","\n","・**モデルの保存先ディレクトリ** (MODEL_SAVE_TO)：学習中にモデルが保存されるディレクトリを指定する．ディレクトリ内に（モデル名 入力解像度 クラス数）の名前のサブディレクトリが作成され，その中にモデルの重みが保存される．\n","\n","・**ネットワークの入力解像度** (IMG_SHAPE)：解像度が高い方が入力情報量が増えるので精度が向上する傾向にあるが，学習時間と識別時間が増える\n","\n","・**ネットワークの入力チャンネル数** (IMG_SHAPE)：3チャンネル（カラー画像）と1チャンネル（グレースケール画像）を選択できる．**1チャンネルを選んだ場合はTensorFlowに内蔵されたImageNetの学習済みモデルで転移学習できないので注意．つまりTRANSFER_FROM_IMAGENETをFalseにし，かつENABLE_FULL_TUNEをTrueにする必要がある．**（TensorFlowに内蔵されたImageNetの学習済みのモデルは3チャンネルで学習しているので，重みを転移できない）\n","\n","・**転移学習の可否** (TRANSFER_FROM_IMAGENET)：モデルの重みをImageNetで学習済みのモデルで初期化するか（転移学習），乱数で初期化するか（フルスクラッチ学習）選択する．\n","\n","・**フルチューニングとファインチューニングの選択** (ENABLE_FULL_TUNE)：フルチューニングを選んだ場合はモデルの重みを全て学習する．ファインチューニングを選んだ場合は出力層の重みだけを学習し，それ以外の層の重みは固定（フリーズ）する．**フルスクラッチ学習を行う場合はフルチューニングを行わないと精度が全く出ないので注意**（出力層以外の重みが乱数で固定されたまま学習されないため）\n","\n","・**バッチサイズ** (BATCH_SIZE)：モデルの1ステップの学習で画像を何枚同時に流し込むか（バッチサイズ）を設定する．バッチサイズが大きいほどモデルの使用するメモリ量が増加する．**GPUのメモリ不足のエラーが起きたらバッチサイズを下げること**．バッチサイズは大きすぎても小さすぎても学習が収束しづらくなる．適切な値に決まった値は無い．\n","\n","・**使用するモデル** (MODEL_NAME)：3.の**MODELS**に定義されたモデルの中から選択する．それぞれのモデルは精度・速度・メモリ使用量が異なる．メモリ使用量が大きいモデル（サイズの大きなモデル）は，一般に低速・高精度の傾向がある．\n","\n","・**学習エポック数** (EPOCHS)：エポック数とは，用意したデータの学習が1周した回数のこと．例えば100枚の画像を学習データとして用意した場合，100枚分の学習が済むと1エポックと数える．学習が収束するまでのエポック数は認識対象のデータや使用モデルによって変わる．\n","\n","・**モデル保存のエポック間隔** (SAVE_PERIOD)：学習中にモデルを保存するエポック数の間隔．モデルは約100MBの容量があるので，1エポックごとに保存すると大容量を消費する．各自の目的に応じて設定する．\n","\n","・**学習アルゴリズム** (OPTIMIZER_NAME)：モデルの重みを更新するアルゴリズムを指定する．3.の**OPTIMIZERS**に定義されたモデルの中から選択する．アルゴリズムによって学習が収束するまでの時間と，学習が収束した際の精度が異なる．\n","\n","・**初期学習率** (INITIAL_LEARNING_RATE)：学習アルゴリズムに設定する初期学習率を指定する．学習率とは「1バッチの学習でモデルの重みをどの程度更新するか」を表す値であり，学習率が高いほどモデルの重みは大きく変動し学習が速く進む傾向にあるが，モデルの重みが微調整できないので最終的に得られる精度は低くなりやすい．**学習率の設定が適切な範囲を大きく外れると学習が全く進まなくなる**．適切な範囲は入力データとモデルに依存する．"]},{"cell_type":"code","metadata":{"id":"93Odo2fW7cAJ","colab_type":"code","colab":{}},"source":["# 学習用画像のディレクトリ．直下にクラス名をディレクトリ名に設定したサブディレクトリを作成する\n","INPUT_DIR = '/content/drive/My Drive/AppliedSeminar2019/SampleData/Caltech101'\n","\n","# 学習経過のモデルの保存先\n","MODEL_SAVE_TO = '/content/drive/My Drive/AppliedSeminar2019/Output'\n","\n","# 訓練セット，検証セット，テストセットの分割率\n","SPLITS = {\n","    'train': 0.70,\n","    'validation': 0.15,\n","    'test': 0.15,\n","}\n","\n","# モデルの入力サイズを指定する（高さ, 幅, チャンネル数）\n","# 学習用画像は指定したサイズにアスペクト比を維持してリサイズされる\n","IMG_SHAPE = (224, 224, 3)\n","\n","# ネットワークの重みの初期値を決める\n","#   True: ImageNetで学習済みの重みを使う\n","#   False: 乱数で初期化する\n","TRANSFER_FROM_IMAGENET = True\n","\n","# モデルの重みの更新方法（フルチューニングとファインチューニング）の選択\n","#   True: フルチューニングを行う．モデル全体の重みを学習する\n","#   False: ファインチューニングを行う．最終層以外の層の重みを固定し，最終層の重みのみ学習する\n","ENABLE_FULL_TUNE = False\n","\n","# バッチサイズ\n","BATCH_SIZE = 32\n","\n","# 使用するモデル\n","MODEL_NAME = 'ResNet50V2'\n","\n","# 学習するエポック数\n","EPOCHS = 10\n","\n","# 学習中にモデルを保存するエポック数の間隔\n","SAVE_PERIOD = 3\n","\n","# 学習アルゴリズム\n","OPTIMIZER_NAME = 'Adam'\n","\n","# 初期学習率\n","INITIAL_LEARNING_RATE = 0.001"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VCflrX5OnYNm","colab_type":"text"},"source":["# 6. ハイパーパラメータのエラーチェック"]},{"cell_type":"code","metadata":{"id":"RXgD9tvClQ1u","colab_type":"code","colab":{}},"source":["if IMG_SHAPE[2] == 1 and TRANSFER_FROM_IMAGENET:\n","  print('エラー：ImageNetで学習済みの重みを使う場合は入力チャンネル数は3にする必要があります')\n","  \n","if not TRANSFER_FROM_IMAGENET and not ENABLE_FULL_TUNE:\n","  print('警告：転移学習しない場合はフルチューニングで全層を学習しないと精度が全く出ません')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xhUsUfEyqffy","colab_type":"text"},"source":["# 7. 学習データの読み込み"]},{"cell_type":"code","metadata":{"id":"ievroQhW8D94","colab_type":"code","colab":{}},"source":["label_names = [item.name for item in pathlib2.Path(INPUT_DIR).iterdir() if item.is_dir()]\n","label_to_index = dict((name, index) for index, name in enumerate(label_names))\n","index_to_label =  dict((index, name) for name, index in label_to_index.items())\n","\n","image_paths = [str(path) for path in pathlib2.Path(INPUT_DIR).glob('*/*')]\n","random.seed(0)\n","random.shuffle(image_paths)\n","seed = random.randrange(sys.maxsize)\n","random.seed(seed)\n","image_labels = [label_to_index[pathlib2.Path(path).parent.name] for path in image_paths]\n","\n","dataset_size = len(image_paths)\n","dataset = tf.data.Dataset.from_tensor_slices((image_paths, image_labels))\n","\n","def path2image(path, label):\n","  raw_bytes = tf.io.read_file(path)\n","  image = tf.zeros((1, 1, 3), tf.uint8)\n","  image = tf.cond(tf.strings.regex_full_match(tf.strings.lower(path), '.*\\.jpg'), lambda: tf.image.decode_jpeg(raw_bytes), lambda:image)\n","  image = tf.cond(tf.strings.regex_full_match(tf.strings.lower(path), '.*\\.jpeg'), lambda: tf.image.decode_jpeg(raw_bytes), lambda: image)\n","  image = tf.cond(tf.strings.regex_full_match(tf.strings.lower(path), '.*\\.png'), lambda: tf.image.decode_png(raw_bytes), lambda: image)\n","  return image, label\n","\n","def preprocess_image(path, label):\n","  image, label = path2image(path, label)\n","  image = tf.image.resize_with_pad(image, IMG_SHAPE[0], IMG_SHAPE[1])\n","  image = tf.slice(tf.tile(image, [1, 1, 3]), [0, 0, 0], [IMG_SHAPE[0], IMG_SHAPE[1], 3])\n","  image = MODELS[MODEL_NAME][1](image)\n","  if (IMG_SHAPE[2] == 1):\n","    image = tf.image.rgb_to_grayscale(image)\n","  return image, label\n","\n","train_size = int(dataset_size * SPLITS['train'])\n","validation_size = int(dataset_size * SPLITS['validation'])\n","test_size = int(dataset_size * SPLITS['test'])\n","\n","train_batches = dataset.take(train_size).shuffle(1).map(preprocess_image).repeat().batch(BATCH_SIZE)\n","validation_batches = dataset.skip(train_size).take(validation_size).shuffle(1).map(preprocess_image).repeat().batch(BATCH_SIZE)\n","test_batches = dataset.skip(train_size + validation_size).take(test_size).shuffle(1).map(preprocess_image).repeat().batch(BATCH_SIZE)\n","\n","print('Labels:', len(label_names))\n","for index, label in index_to_label.items():\n","  print('  %d: %s' % (index, label))\n","print()\n","print('All Images:', len(image_paths))\n","print('  Train Images:', train_size)\n","print('  Validation Images:', validation_size)\n","print('  Test Images:', test_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QqG2n4KqpuZ","colab_type":"text"},"source":["# 8. 学習データをランダムに表示\n","\n","前処理の適用前と適用後の画像をそれぞれ表示．\n","\n","**前処理**：モデルには入力画像のRGB値をそのまま入力せずに，予め定数倍や定数減算などの前処理を施した方が高い精度が得られることが知られている．**転移学習する場合は転移元のモデルと同じ前処理を実行してから学習や推論しないと精度が全く出ない**"]},{"cell_type":"code","metadata":{"id":"fDzx0HM1pbpE","colab_type":"code","colab":{}},"source":["index = random.randint(0, dataset_size)\n","for (image, label), (image2, label2) in zip(dataset.skip(index).take(1).map(path2image), dataset.skip(index).take(1).map(preprocess_image)):\n","  plt.figure()\n","  image = (image.numpy()).astype(np.uint8)\n","  if image.shape[2] == 1:\n","    image = np.squeeze(np.stack((image,) * 3, axis=-1))\n","  plt.imshow(image)\n","  plt.title(index_to_label[label2.numpy()])\n","\n","  plt.figure()\n","  image2 = (image2.numpy()).astype(np.uint8)\n","  if image2.shape[2] == 1:\n","    image2 = np.squeeze(np.stack((image2,) * 3, axis=-1))\n","  plt.imshow(image2)\n","  plt.title(index_to_label[label2.numpy()])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1w_rszItq-Rc","colab_type":"text"},"source":["# 9. DNNモデルの構築\n","\n","モデルをメモリ上に構築する．モデルの出力ディレクトリに（モデル名 入力解像度 クラス数）のサブディレクトリを作成し，さらにその中にモデルの構造を示したbase-model.pngとmodel.pngの画像を出力する．"]},{"cell_type":"code","metadata":{"id":"JoL379EA-3xj","colab_type":"code","colab":{}},"source":["output_dir = pathlib2.Path(MODEL_SAVE_TO) / ('%s (%dx%dx%d) (%d classes)' % (MODEL_NAME, IMG_SHAPE[0], IMG_SHAPE[1], IMG_SHAPE[2], len(index_to_label)))\n","os.makedirs(str(output_dir), exist_ok=True)\n","\n","def create_model():\n","  base_model = MODELS[MODEL_NAME][0](input_shape=IMG_SHAPE, include_top=False, weights='imagenet' if TRANSFER_FROM_IMAGENET else None)\n","  base_model.trainable = ENABLE_FULL_TUNE\n","\n","  # base_model.summary()\n","  plot_model(base_model, to_file=str(output_dir / 'base-model.png'), show_shapes=True, show_layer_names=False)\n","  cv2.imshow('Model', cv2.imread(str(output_dir / 'base-model.png')))\n","  \n","  model = tf.keras.Sequential([\n","    base_model,\n","    tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.layers.Dense(len(label_names), activation='softmax'),\n","  ])\n","  \n","  model.compile(optimizer=OPTIMIZERS[OPTIMIZER_NAME](learning_rate=INITIAL_LEARNING_RATE),\n","                loss='sparse_categorical_crossentropy',\n","                metrics=[\"accuracy\"])\n","  \n","  plot_model(model, to_file=str(output_dir / 'full-model.png'), show_shapes=True, show_layer_names=False)\n","  # base_model.summary()\n","  return model\n","\n","model = create_model()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3naAzjT9rCht","colab_type":"text"},"source":["# 10. 学習の実行\n","\n","学習を行い，その途中経過をGoogle Driveの指定されたディレクトリに出力する．**学習はいつでも途中から再開できる**．"]},{"cell_type":"code","metadata":{"id":"JrKcuU5tADlz","colab_type":"code","colab":{}},"source":["latest = tf.train.latest_checkpoint(str(output_dir))\n","if latest is not None:\n","  model.load_weights(latest)\n","\n","# steps_per_epoch = round(train_size)//BATCH_SIZE\n","train_steps = int(train_size/BATCH_SIZE)\n","validation_steps = int(validation_size/BATCH_SIZE)\n","test_steps = int(test_size/BATCH_SIZE)\n","\n","print('Last checkpoint = %s' % latest)\n","initial_epoch = 0\n","if latest is not None:\n","  latest = tf.train.latest_checkpoint(str(output_dir))\n","  match = re.search('model.ckpt-(\\\\d+)$', latest)\n","  initial_epoch = int(match.group(1))\n","else:\n","  loss, acc = model.evaluate(train_batches, steps=train_steps)\n","  val_loss, val_acc = model.evaluate(validation_batches, steps=validation_steps)\n","  model.save_weights(str(output_dir / 'model.ckpt-00000'))\n","  history = {\n","    'loss': [loss],\n","    'accuracy': [acc],\n","    'val_loss': [val_loss],\n","    'val_accuracy': [val_acc],\n","  }\n","  with open(str(output_dir / 'history'), 'wb') as file_pi:\n","    pickle.dump(history, file_pi)\n","\n","checkpoint_path = str(output_dir / 'model.ckpt-{epoch:05d}')\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    checkpoint_path,\n","    verbose=1,\n","    save_weights_only=True,\n","    save_freq=SAVE_PERIOD * BATCH_SIZE * train_steps)\n","\n","class MyCustomCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, initial_epoch, period):\n","    self.epoch = initial_epoch + 1\n","    self.period = period\n","    with open(str(output_dir / 'history'), 'rb') as file_pi:\n","      self.history = pickle.load(file_pi)\n","  \n","  def on_epoch_end(self, batch, logs=None):\n","    self.history['loss'].append(logs['loss'])\n","    self.history['accuracy'].append(logs['accuracy'])\n","    self.history['val_loss'].append(logs['val_loss'])\n","    self.history['val_accuracy'].append(logs['val_accuracy'])\n","    if (self.epoch % self.period) == 0:\n","      with open(str(output_dir / 'history'), 'wb') as file_pi:\n","        pickle.dump(self.history, file_pi)\n","    self.epoch += 1\n","\n","print('Initial_epoch=%d' % initial_epoch)\n","history = model.fit(train_batches,\n","                    initial_epoch=initial_epoch,\n","                    epochs=EPOCHS,\n","                    validation_data=validation_batches,\n","                    validation_steps=validation_steps,\n","                    callbacks=[cp_callback, MyCustomCallback(initial_epoch, SAVE_PERIOD)],\n","                    steps_per_epoch=train_steps)\n","\n","loss, acc = model.evaluate(test_batches, steps=test_steps)\n","print('学習後のモデルの精度')\n","print(\"loss: {:.2f}\".format(loss))\n","print(\"accuracy: {:.2f}\".format(acc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LU_24uysrFVU","colab_type":"text"},"source":["# 11. 学習中の精度とロスの推移をグラフに表示する\n","\n","・**精度**：DNNの良し悪しを人間が判断するための評価値．例えば二値分類器における識別精度では，Aの確率90%と答えて間違えた場合と，Aの確率51%と答えて間違えた場合も，どちらも「1回の間違い」と数えられる．\n","\n","・**ロス**：DNNの良し悪しを最適化アルゴリズムが判断するための評価値．例えば二値分類器における識別ロスでは，Aの確率90%と答えて間違えた場合と，Aの確率51%と答えて間違えた場合は，ロスの値は確率90%と答えた方が高くなる．"]},{"cell_type":"code","metadata":{"id":"czrHK_LlQUQO","colab_type":"code","colab":{}},"source":["history2 = None\n","with open(str(output_dir / 'history'), 'rb') as file_pi:\n","  history2 = pickle.load(file_pi)\n","\n","acc = history2['accuracy']\n","val_acc = history2['val_accuracy']\n","\n","loss = history2['loss']\n","val_loss = history2['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n","plt.ylabel('Accuracy')\n","plt.ylim([0, 1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n","plt.ylabel('Cross Entropy')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gm-p2m-FrOUt","colab_type":"text"},"source":["# 12. 学習後のモデルの動作確認\n","\n","**INPUT_IMAGE_PATH**で指定した画像に対し推論を実行する．INPUT_IMAGE_PATHがNoneの場合はテストセットからランダムに1枚画像を取り出して推論を実行する．\n","\n","**Ground Truth**：正解クラスのこと"]},{"cell_type":"code","metadata":{"id":"pm4lma0gr67U","colab_type":"code","colab":{}},"source":["latest = tf.train.latest_checkpoint(str(output_dir))\n","model.load_weights(latest)\n","\n","INPUT_IMAGE_PATH = None\n","# INPUT_IMAGE_PATH = '/content/drive/My Drive/AppliedSeminar2019/SampleData/Caltech101/camera/image_0001.jpg'\n","\n","if INPUT_IMAGE_PATH == None:\n","  path, gt_index = random.choice(list(zip(image_paths, image_labels))[train_size + validation_size:])\n","  gt_label = index_to_label[gt_index]\n","else:\n","  path = INPUT_IMAGE_PATH\n","  gt_label = pathlib2.Path(INPUT_IMAGE_PATH).parent.name\n","\n","image, label = path2image(path, gt_label)  \n","plt.figure()\n","np_image = (image.numpy()).astype(np.uint8)\n","if np_image.shape[2] == 1:\n","  np_image = np.squeeze(np.stack((np_image,) * 3, axis=-1))\n","plt.imshow(np_image)\n","plt.title(label)\n","\n","image, label = preprocess_image(path, gt_label)\n","plt.figure()\n","np_image = (image.numpy()).astype(np.uint8)\n","if np_image.shape[2] == 1:\n","  np_image = np.squeeze(np.stack((np_image,) * 3, axis=-1))\n","plt.imshow(np_image)\n","plt.title(label)\n","\n","image = tf.expand_dims(image, axis=0)\n","preds = model.predict(image, steps=1)\n","pred = preds[0]\n","\n","top = len(index_to_label)\n","top_indices = pred.argsort()[-top:][::-1]\n","result = [tuple([index_to_label[i], pred[i]]) for i in top_indices]\n","result.sort(key=lambda x: x[1], reverse=True)\n","\n","print('Predicted scores')\n","for index, (name, confidence) in enumerate(result):\n","  if name == gt_label:\n","    print('  [%d] \"%s\": %f (Ground truth)' % (index + 1, name, confidence))\n","  else:\n","    print('  [%d] \"%s\": %f' % (index + 1, name, confidence))"],"execution_count":0,"outputs":[]}]}